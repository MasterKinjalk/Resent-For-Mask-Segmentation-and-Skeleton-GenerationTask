{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLyU2Y60KRbk",
    "outputId": "61e14c48-7adc-4f77-9eff-6a9ed5c2341b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09yydkSeKsC6",
    "outputId": "d6ec79d7-d4a8-4cd5-de3e-f8edf3b55faf"
   },
   "outputs": [],
   "source": [
    "# # prompt: take me to the RA Data folder\n",
    "\n",
    "# %cd /content/drive/MyDrive/RA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KRAKBzIhJexF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from PIL import Image\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0mfBMJzDJexG"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WKRbjDniJexH"
   },
   "outputs": [],
   "source": [
    "# class DualTaskResNet(nn.Module):\n",
    "#     def __init__(self, num_classes=1, pretrained=True):\n",
    "#         super(DualTaskResNet, self).__init__()\n",
    "#         # Load a pretrained ResNet\n",
    "#         self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "#         # Remove the last fully connected layer\n",
    "#         self.features = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "\n",
    "#         # Upsampling layers for mask generation\n",
    "#         self.upsample1 = self._make_upsample_block(2048, 1024)\n",
    "#         self.upsample2 = self._make_upsample_block(1024, 512)\n",
    "#         self.upsample3 = self._make_upsample_block(512, 256)\n",
    "#         self.upsample4 = self._make_upsample_block(256, 64)\n",
    "#         self.upsample5 = self._make_upsample_block(64, num_classes, final=True)\n",
    "\n",
    "#         # Enhanced skeletonization layers\n",
    "#         self.skeleton_decoder = nn.Sequential(\n",
    "#             nn.Conv2d(num_classes, 32, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             ResidualBlock(32),\n",
    "#             nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             ResidualBlock(16),\n",
    "#             nn.Conv2d(16, num_classes, kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#     def _make_upsample_block(self, in_channels, out_channels, final=False):\n",
    "#         layers = []\n",
    "#         for i in range(3):  # Stack 3 ConvTranspose2d layers\n",
    "#             if i == 0:\n",
    "#                 layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2))\n",
    "#             else:\n",
    "#                 layers.append(nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "#             if not final:\n",
    "#                 layers.append(nn.BatchNorm2d(out_channels))\n",
    "#                 layers.append(nn.ReLU(inplace=True))\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x, task='mask'):\n",
    "#         x = self.features(x)\n",
    "#         x = self.upsample1(x)\n",
    "#         x = self.upsample2(x)\n",
    "#         x = self.upsample3(x)\n",
    "#         x = self.upsample4(x)\n",
    "#         mask = self.upsample5(x)\n",
    "\n",
    "#         if task == 'mask':\n",
    "#             return torch.sigmoid(mask)\n",
    "#         elif task == 'skeleton':\n",
    "#             skeleton = self.skeleton_decoder(mask)\n",
    "#             return torch.sigmoid(skeleton)\n",
    "\n",
    "#     def freeze_encoder(self):\n",
    "#         for param in self.features.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def unfreeze_encoder(self):\n",
    "#         for param in self.features.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     def freeze_decoder(self):\n",
    "#         for layer in [self.upsample1, self.upsample2, self.upsample3, self.upsample4, self.upsample5]:\n",
    "#             for param in layer.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#     def unfreeze_decoder(self):\n",
    "#         for layer in [self.upsample1, self.upsample2, self.upsample3, self.upsample4, self.upsample5]:\n",
    "#             for param in layer.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#     def finetune_for_skeleton(self):\n",
    "#         # Freeze the encoder and decoder\n",
    "#         self.freeze_encoder()\n",
    "#         self.freeze_decoder()\n",
    "\n",
    "#         # Unfreeze only the skeleton decoder\n",
    "#         for param in self.skeleton_decoder.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     def prepare_for_full_training(self):\n",
    "#         # Unfreeze all layers for full training\n",
    "#         self.unfreeze_encoder()\n",
    "#         self.unfreeze_decoder()\n",
    "#         for param in self.skeleton_decoder.parameters():\n",
    "#             param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iSNQSiyfyIst"
   },
   "outputs": [],
   "source": [
    "class DualTaskResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True, task = \"mask\"):\n",
    "        super(DualTaskResNet, self).__init__()\n",
    "        # Load a pretrained ResNet\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "\n",
    "        # Remove the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "\n",
    "        # Upsampling layers for output generation\n",
    "        self.upsample1 = self._make_upsample_block(2048, 1024)\n",
    "        self.upsample2 = self._make_upsample_block(1024, 512)\n",
    "        self.upsample3 = self._make_upsample_block(512, 256)\n",
    "        self.upsample4 = self._make_upsample_block(256, 64)\n",
    "        self.upsample5 = self._make_upsample_block(64, num_classes, final=True)\n",
    "\n",
    "        # Task-specific output layers\n",
    "        self.mask_output = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.skeleton_output = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "\n",
    "    def _make_upsample_block(self, in_channels, out_channels, final=False):\n",
    "        layers = []\n",
    "        for i in range(3):  # Stack 3 ConvTranspose2d layers\n",
    "            if i == 0:\n",
    "                layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2))\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            if not final:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, task='mask'):\n",
    "        x = self.features(x)\n",
    "        x = self.upsample1(x)\n",
    "        x = self.upsample2(x)\n",
    "        x = self.upsample3(x)\n",
    "        x = self.upsample4(x)\n",
    "        x = self.upsample5(x)\n",
    "\n",
    "        if task == 'mask':\n",
    "            output = self.mask_output(x)\n",
    "        elif task == 'skeleton':\n",
    "            output = self.skeleton_output(x)\n",
    "        else:\n",
    "            raise ValueError(\"Task must be either 'mask' or 'skeleton'\")\n",
    "\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "    def prepare_for_full_training(self):\n",
    "        # Ensure all parameters are trainable\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def prepare_for_skeleton_finetuning(self):\n",
    "        # Freeze all layers except the skeleton output layer\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.skeleton_output.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jz1KFBQVJexI"
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_dualtask_resnet():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model instance\n",
    "    model = DualTaskResNet(num_classes=1, pretrained=False).to(device)\n",
    "\n",
    "    # Print model summary\n",
    "    summary(model, (3, 224, 224))\n",
    "\n",
    "    # Generate random input\n",
    "    batch_size = 4\n",
    "    input_tensor = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "    # Test mask generation\n",
    "    print(\"\\nTesting mask generation:\")\n",
    "    mask_output = model(input_tensor, task='mask')\n",
    "    print(f\"Mask output shape: {mask_output.shape}\")\n",
    "    print(f\"Mask output min: {mask_output.min().item():.4f}, max: {mask_output.max().item():.4f}\")\n",
    "\n",
    "    # Test skeletonization\n",
    "    print(\"\\nTesting skeletonization:\")\n",
    "    skeleton_output = model(input_tensor, task='skeleton')\n",
    "    print(f\"Skeleton output shape: {skeleton_output.shape}\")\n",
    "    print(f\"Skeleton output min: {skeleton_output.min().item():.4f}, max: {skeleton_output.max().item():.4f}\")\n",
    "\n",
    "    # Test if outputs are different\n",
    "    print(\"\\nChecking if mask and skeleton outputs are different:\")\n",
    "    is_different = not torch.allclose(mask_output, skeleton_output)\n",
    "    print(f\"Outputs are different: {is_different}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnDzZCBrJexI",
    "outputId": "f03ac0b9-d05c-4bc6-a6fc-8b82de942f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-2         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "       BatchNorm2d-4         [-1, 64, 112, 112]             128\n",
      "              ReLU-5         [-1, 64, 112, 112]               0\n",
      "              ReLU-6         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-7           [-1, 64, 56, 56]               0\n",
      "         MaxPool2d-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]           4,096\n",
      "           Conv2d-10           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "           Conv2d-21          [-1, 256, 56, 56]          16,384\n",
      "           Conv2d-22          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "           Conv2d-25          [-1, 256, 56, 56]          16,384\n",
      "           Conv2d-26          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-27          [-1, 256, 56, 56]             512\n",
      "      BatchNorm2d-28          [-1, 256, 56, 56]             512\n",
      "             ReLU-29          [-1, 256, 56, 56]               0\n",
      "             ReLU-30          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-31          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-32          [-1, 256, 56, 56]               0\n",
      "           Conv2d-33           [-1, 64, 56, 56]          16,384\n",
      "           Conv2d-34           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
      "             ReLU-37           [-1, 64, 56, 56]               0\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "           Conv2d-39           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-40           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-42           [-1, 64, 56, 56]             128\n",
      "             ReLU-43           [-1, 64, 56, 56]               0\n",
      "             ReLU-44           [-1, 64, 56, 56]               0\n",
      "           Conv2d-45          [-1, 256, 56, 56]          16,384\n",
      "           Conv2d-46          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-47          [-1, 256, 56, 56]             512\n",
      "      BatchNorm2d-48          [-1, 256, 56, 56]             512\n",
      "             ReLU-49          [-1, 256, 56, 56]               0\n",
      "             ReLU-50          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-51          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-52          [-1, 256, 56, 56]               0\n",
      "           Conv2d-53           [-1, 64, 56, 56]          16,384\n",
      "           Conv2d-54           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-55           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-56           [-1, 64, 56, 56]             128\n",
      "             ReLU-57           [-1, 64, 56, 56]               0\n",
      "             ReLU-58           [-1, 64, 56, 56]               0\n",
      "           Conv2d-59           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-60           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-61           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-62           [-1, 64, 56, 56]             128\n",
      "             ReLU-63           [-1, 64, 56, 56]               0\n",
      "             ReLU-64           [-1, 64, 56, 56]               0\n",
      "           Conv2d-65          [-1, 256, 56, 56]          16,384\n",
      "           Conv2d-66          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-67          [-1, 256, 56, 56]             512\n",
      "      BatchNorm2d-68          [-1, 256, 56, 56]             512\n",
      "             ReLU-69          [-1, 256, 56, 56]               0\n",
      "             ReLU-70          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-71          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-72          [-1, 256, 56, 56]               0\n",
      "           Conv2d-73          [-1, 128, 56, 56]          32,768\n",
      "           Conv2d-74          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-75          [-1, 128, 56, 56]             256\n",
      "      BatchNorm2d-76          [-1, 128, 56, 56]             256\n",
      "             ReLU-77          [-1, 128, 56, 56]               0\n",
      "             ReLU-78          [-1, 128, 56, 56]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-80          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-81          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
      "             ReLU-83          [-1, 128, 28, 28]               0\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "           Conv2d-86          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-87          [-1, 512, 28, 28]           1,024\n",
      "      BatchNorm2d-88          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-89          [-1, 512, 28, 28]         131,072\n",
      "           Conv2d-90          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-91          [-1, 512, 28, 28]           1,024\n",
      "      BatchNorm2d-92          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-93          [-1, 512, 28, 28]               0\n",
      "             ReLU-94          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-95          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-96          [-1, 512, 28, 28]               0\n",
      "           Conv2d-97          [-1, 128, 28, 28]          65,536\n",
      "           Conv2d-98          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-99          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "            ReLU-102          [-1, 128, 28, 28]               0\n",
      "          Conv2d-103          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-104          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-106          [-1, 128, 28, 28]             256\n",
      "            ReLU-107          [-1, 128, 28, 28]               0\n",
      "            ReLU-108          [-1, 128, 28, 28]               0\n",
      "          Conv2d-109          [-1, 512, 28, 28]          65,536\n",
      "          Conv2d-110          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-111          [-1, 512, 28, 28]           1,024\n",
      "     BatchNorm2d-112          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-113          [-1, 512, 28, 28]               0\n",
      "            ReLU-114          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-115          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-116          [-1, 512, 28, 28]               0\n",
      "          Conv2d-117          [-1, 128, 28, 28]          65,536\n",
      "          Conv2d-118          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-119          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-120          [-1, 128, 28, 28]             256\n",
      "            ReLU-121          [-1, 128, 28, 28]               0\n",
      "            ReLU-122          [-1, 128, 28, 28]               0\n",
      "          Conv2d-123          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-124          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-125          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-126          [-1, 128, 28, 28]             256\n",
      "            ReLU-127          [-1, 128, 28, 28]               0\n",
      "            ReLU-128          [-1, 128, 28, 28]               0\n",
      "          Conv2d-129          [-1, 512, 28, 28]          65,536\n",
      "          Conv2d-130          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-131          [-1, 512, 28, 28]           1,024\n",
      "     BatchNorm2d-132          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-133          [-1, 512, 28, 28]               0\n",
      "            ReLU-134          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-135          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-136          [-1, 512, 28, 28]               0\n",
      "          Conv2d-137          [-1, 128, 28, 28]          65,536\n",
      "          Conv2d-138          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-139          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-140          [-1, 128, 28, 28]             256\n",
      "            ReLU-141          [-1, 128, 28, 28]               0\n",
      "            ReLU-142          [-1, 128, 28, 28]               0\n",
      "          Conv2d-143          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-144          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-145          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-146          [-1, 128, 28, 28]             256\n",
      "            ReLU-147          [-1, 128, 28, 28]               0\n",
      "            ReLU-148          [-1, 128, 28, 28]               0\n",
      "          Conv2d-149          [-1, 512, 28, 28]          65,536\n",
      "          Conv2d-150          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-151          [-1, 512, 28, 28]           1,024\n",
      "     BatchNorm2d-152          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-153          [-1, 512, 28, 28]               0\n",
      "            ReLU-154          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-155          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-156          [-1, 512, 28, 28]               0\n",
      "          Conv2d-157          [-1, 256, 28, 28]         131,072\n",
      "          Conv2d-158          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-159          [-1, 256, 28, 28]             512\n",
      "     BatchNorm2d-160          [-1, 256, 28, 28]             512\n",
      "            ReLU-161          [-1, 256, 28, 28]               0\n",
      "            ReLU-162          [-1, 256, 28, 28]               0\n",
      "          Conv2d-163          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-166          [-1, 256, 14, 14]             512\n",
      "            ReLU-167          [-1, 256, 14, 14]               0\n",
      "            ReLU-168          [-1, 256, 14, 14]               0\n",
      "          Conv2d-169         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-170         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-171         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-172         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-173         [-1, 1024, 14, 14]         524,288\n",
      "          Conv2d-174         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-175         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-176         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-177         [-1, 1024, 14, 14]               0\n",
      "            ReLU-178         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "          Conv2d-182          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-183          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-184          [-1, 256, 14, 14]             512\n",
      "            ReLU-185          [-1, 256, 14, 14]               0\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-188          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-189          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-190          [-1, 256, 14, 14]             512\n",
      "            ReLU-191          [-1, 256, 14, 14]               0\n",
      "            ReLU-192          [-1, 256, 14, 14]               0\n",
      "          Conv2d-193         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-194         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-195         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-196         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-197         [-1, 1024, 14, 14]               0\n",
      "            ReLU-198         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "          Conv2d-202          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-203          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-204          [-1, 256, 14, 14]             512\n",
      "            ReLU-205          [-1, 256, 14, 14]               0\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-208          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-209          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-210          [-1, 256, 14, 14]             512\n",
      "            ReLU-211          [-1, 256, 14, 14]               0\n",
      "            ReLU-212          [-1, 256, 14, 14]               0\n",
      "          Conv2d-213         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-214         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-215         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-216         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-217         [-1, 1024, 14, 14]               0\n",
      "            ReLU-218         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "          Conv2d-222          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-223          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-224          [-1, 256, 14, 14]             512\n",
      "            ReLU-225          [-1, 256, 14, 14]               0\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-228          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-229          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-230          [-1, 256, 14, 14]             512\n",
      "            ReLU-231          [-1, 256, 14, 14]               0\n",
      "            ReLU-232          [-1, 256, 14, 14]               0\n",
      "          Conv2d-233         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-234         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-235         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-236         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-237         [-1, 1024, 14, 14]               0\n",
      "            ReLU-238         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "          Conv2d-242          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-243          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-244          [-1, 256, 14, 14]             512\n",
      "            ReLU-245          [-1, 256, 14, 14]               0\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-248          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-249          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-250          [-1, 256, 14, 14]             512\n",
      "            ReLU-251          [-1, 256, 14, 14]               0\n",
      "            ReLU-252          [-1, 256, 14, 14]               0\n",
      "          Conv2d-253         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-254         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-255         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-256         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-257         [-1, 1024, 14, 14]               0\n",
      "            ReLU-258         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "          Conv2d-262          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-263          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-264          [-1, 256, 14, 14]             512\n",
      "            ReLU-265          [-1, 256, 14, 14]               0\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-268          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-269          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-270          [-1, 256, 14, 14]             512\n",
      "            ReLU-271          [-1, 256, 14, 14]               0\n",
      "            ReLU-272          [-1, 256, 14, 14]               0\n",
      "          Conv2d-273         [-1, 1024, 14, 14]         262,144\n",
      "          Conv2d-274         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-275         [-1, 1024, 14, 14]           2,048\n",
      "     BatchNorm2d-276         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-277         [-1, 1024, 14, 14]               0\n",
      "            ReLU-278         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 512, 14, 14]         524,288\n",
      "          Conv2d-282          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-283          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-284          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-285          [-1, 512, 14, 14]               0\n",
      "            ReLU-286          [-1, 512, 14, 14]               0\n",
      "          Conv2d-287            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-288            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-289            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-290            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-291            [-1, 512, 7, 7]               0\n",
      "            ReLU-292            [-1, 512, 7, 7]               0\n",
      "          Conv2d-293           [-1, 2048, 7, 7]       1,048,576\n",
      "          Conv2d-294           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-295           [-1, 2048, 7, 7]           4,096\n",
      "     BatchNorm2d-296           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-297           [-1, 2048, 7, 7]       2,097,152\n",
      "          Conv2d-298           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-299           [-1, 2048, 7, 7]           4,096\n",
      "     BatchNorm2d-300           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-301           [-1, 2048, 7, 7]               0\n",
      "            ReLU-302           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-303           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-304           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-305            [-1, 512, 7, 7]       1,048,576\n",
      "          Conv2d-306            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-307            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-308            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-309            [-1, 512, 7, 7]               0\n",
      "            ReLU-310            [-1, 512, 7, 7]               0\n",
      "          Conv2d-311            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-312            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-313            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-314            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-315            [-1, 512, 7, 7]               0\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "          Conv2d-318           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-319           [-1, 2048, 7, 7]           4,096\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "            ReLU-322           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-323           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-324           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-325            [-1, 512, 7, 7]       1,048,576\n",
      "          Conv2d-326            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-328            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-329            [-1, 512, 7, 7]               0\n",
      "            ReLU-330            [-1, 512, 7, 7]               0\n",
      "          Conv2d-331            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-332            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-333            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "            ReLU-336            [-1, 512, 7, 7]               0\n",
      "          Conv2d-337           [-1, 2048, 7, 7]       1,048,576\n",
      "          Conv2d-338           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-339           [-1, 2048, 7, 7]           4,096\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "            ReLU-342           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-343           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-344           [-1, 2048, 7, 7]               0\n",
      " ConvTranspose2d-345         [-1, 1024, 14, 14]       8,389,632\n",
      "     BatchNorm2d-346         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-347         [-1, 1024, 14, 14]               0\n",
      " ConvTranspose2d-348         [-1, 1024, 14, 14]       9,438,208\n",
      "     BatchNorm2d-349         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-350         [-1, 1024, 14, 14]               0\n",
      " ConvTranspose2d-351         [-1, 1024, 14, 14]       9,438,208\n",
      "     BatchNorm2d-352         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-353         [-1, 1024, 14, 14]               0\n",
      " ConvTranspose2d-354          [-1, 512, 28, 28]       2,097,664\n",
      "     BatchNorm2d-355          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-356          [-1, 512, 28, 28]               0\n",
      " ConvTranspose2d-357          [-1, 512, 28, 28]       2,359,808\n",
      "     BatchNorm2d-358          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-359          [-1, 512, 28, 28]               0\n",
      " ConvTranspose2d-360          [-1, 512, 28, 28]       2,359,808\n",
      "     BatchNorm2d-361          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-362          [-1, 512, 28, 28]               0\n",
      " ConvTranspose2d-363          [-1, 256, 56, 56]         524,544\n",
      "     BatchNorm2d-364          [-1, 256, 56, 56]             512\n",
      "            ReLU-365          [-1, 256, 56, 56]               0\n",
      " ConvTranspose2d-366          [-1, 256, 56, 56]         590,080\n",
      "     BatchNorm2d-367          [-1, 256, 56, 56]             512\n",
      "            ReLU-368          [-1, 256, 56, 56]               0\n",
      " ConvTranspose2d-369          [-1, 256, 56, 56]         590,080\n",
      "     BatchNorm2d-370          [-1, 256, 56, 56]             512\n",
      "            ReLU-371          [-1, 256, 56, 56]               0\n",
      " ConvTranspose2d-372         [-1, 64, 112, 112]          65,600\n",
      "     BatchNorm2d-373         [-1, 64, 112, 112]             128\n",
      "            ReLU-374         [-1, 64, 112, 112]               0\n",
      " ConvTranspose2d-375         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-376         [-1, 64, 112, 112]             128\n",
      "            ReLU-377         [-1, 64, 112, 112]               0\n",
      " ConvTranspose2d-378         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-379         [-1, 64, 112, 112]             128\n",
      "            ReLU-380         [-1, 64, 112, 112]               0\n",
      " ConvTranspose2d-381          [-1, 1, 224, 224]             257\n",
      " ConvTranspose2d-382          [-1, 1, 224, 224]              10\n",
      " ConvTranspose2d-383          [-1, 1, 224, 224]              10\n",
      "          Conv2d-384          [-1, 1, 224, 224]               2\n",
      "================================================================\n",
      "Total params: 82,954,967\n",
      "Trainable params: 82,954,967\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 726.20\n",
      "Params size (MB): 316.45\n",
      "Estimated Total Size (MB): 1043.22\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Testing mask generation:\n",
      "Mask output shape: torch.Size([4, 1, 224, 224])\n",
      "Mask output min: 0.0669, max: 0.9096\n",
      "\n",
      "Testing skeletonization:\n",
      "Skeleton output shape: torch.Size([4, 1, 224, 224])\n",
      "Skeleton output min: 0.0159, max: 0.6555\n",
      "\n",
      "Checking if mask and skeleton outputs are different:\n",
      "Outputs are different: True\n"
     ]
    }
   ],
   "source": [
    "test_dualtask_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2nxlB692JexJ"
   },
   "outputs": [],
   "source": [
    "class DualTaskDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, skeleton_dir, image_transform=None, mask_transform=None):\n",
    "        self.image_dir = os.path.normpath(image_dir)\n",
    "        self.mask_dir = os.path.normpath(mask_dir)\n",
    "        self.skeleton_dir = os.path.normpath(skeleton_dir)\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.jpeg') or f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.normpath(os.path.join(self.image_dir, img_name))\n",
    "\n",
    "        # Construct the mask and skeleton paths\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        mask_name = base_name + '.png'\n",
    "        skeleton_name = base_name + '.png'\n",
    "        mask_path = os.path.normpath(os.path.join(self.mask_dir, mask_name))\n",
    "        skeleton_path = os.path.normpath(os.path.join(self.skeleton_dir, skeleton_name))\n",
    "\n",
    "        # Load the images\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        skeleton = Image.open(skeleton_path).convert('L')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            skeleton = self.mask_transform(skeleton)\n",
    "\n",
    "        return image, mask, skeleton\n",
    "\n",
    "\n",
    "def get_data_loaders(image_dir, mask_dir, skeleton_dir, batch_size=32, train_split=0.7, val_split=0.15, test_split=0.15):\n",
    "    # Define transforms for input images\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5514, 0.4094, 0.3140], std=[0.1299, 0.1085, 0.0914])\n",
    "    ])\n",
    "\n",
    "    # Define transforms for masks and skeletons\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create dataset\n",
    "    full_dataset = DualTaskDataset(image_dir, mask_dir, skeleton_dir,\n",
    "                                   image_transform=image_transform,\n",
    "                                   mask_transform=mask_transform)\n",
    "\n",
    "    # Calculate split sizes\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = int(val_split * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCr9zTeVJexJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I5okkVn2JexJ"
   },
   "outputs": [],
   "source": [
    "def linear_loss_decay(epoch, total_epochs, start_weight=0.5, end_weight=1.0):\n",
    "    return start_weight + (end_weight - start_weight) * (epoch / total_epochs)\n",
    "\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)  # Adding small epsilon to avoid division by zero\n",
    "\n",
    "def calculate_pos_weight(train_loader):\n",
    "    total_pixels = 0\n",
    "    skeleton_pixels = 0\n",
    "    for _, _, skeletons in train_loader:\n",
    "        total_pixels += skeletons.numel()\n",
    "        skeleton_pixels += skeletons.sum().item()\n",
    "    neg_pos_ratio = (total_pixels - skeleton_pixels) / skeleton_pixels\n",
    "    return torch.tensor([neg_pos_ratio])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QWYq2G1IJexJ"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device, task):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iou_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, skeletons in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = masks.to(device) if task == 'mask' else skeletons.to(device)\n",
    "\n",
    "            outputs = model(images, task=task)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            iou_sum += calculate_iou(outputs, targets)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_iou = iou_sum / len(val_loader)\n",
    "\n",
    "    return val_loss, val_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KX7oTHNxJexK"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_with_finetuning(model, train_loader, val_loader, num_epochs, device):\n",
    "    best_mask_iou = 0.0\n",
    "    best_skeleton_iou = 0.0\n",
    "    best_mask_model_weights = None\n",
    "    best_skeleton_model_weights = None\n",
    "\n",
    "    # Data collection for visualization\n",
    "    mask_train_losses, mask_val_losses, mask_ious, mask_lrs = [], [], [], []\n",
    "    skeleton_train_losses, skeleton_val_losses, skeleton_ious, skeleton_lrs = [], [], [], []\n",
    "\n",
    "    # Phase 1: Train mask layers\n",
    "    print(\"Phase 1: Training mask layers\")\n",
    "    model.prepare_for_full_training()\n",
    "\n",
    "    criterion_mask = nn.BCELoss()\n",
    "    optimizer_mask = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    mask_phase_epochs = num_epochs // 2\n",
    "    constant_lr_epochs = mask_phase_epochs // 2\n",
    "    cosine_annealing_epochs = mask_phase_epochs - constant_lr_epochs\n",
    "\n",
    "    scheduler_mask = torch.optim.lr_scheduler.LambdaLR(optimizer_mask, lambda epoch: 1)  # Constant LR\n",
    "\n",
    "    for epoch in tqdm(range(mask_phase_epochs), desc=\"Mask Training\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, masks, _ in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer_mask.zero_grad()\n",
    "\n",
    "            outputs_mask = model(images)\n",
    "            loss = criterion_mask(outputs_mask, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_mask.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Switch to cosine annealing after half the epochs\n",
    "        if epoch == constant_lr_epochs - 1:\n",
    "            scheduler_mask = CosineAnnealingLR(optimizer_mask, T_max=cosine_annealing_epochs)\n",
    "        scheduler_mask.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss, mask_iou = validate(model, val_loader, criterion_mask, device, task='mask')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{mask_phase_epochs}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Mask IoU: {mask_iou:.4f}, '\n",
    "              f'LR: {scheduler_mask.get_last_lr()[0]:.6f}')\n",
    "\n",
    "        # Collect data for visualization\n",
    "        mask_train_losses.append(train_loss)\n",
    "        mask_val_losses.append(val_loss)\n",
    "        mask_ious.append(mask_iou)\n",
    "        mask_lrs.append(scheduler_mask.get_last_lr()[0])\n",
    "\n",
    "        if mask_iou > best_mask_iou:\n",
    "            best_mask_iou = mask_iou\n",
    "            best_mask_model_weights = model.state_dict().copy()\n",
    "            print(f'New best mask model saved with IoU: {best_mask_iou:.4f}')\n",
    "\n",
    "    # Phase 2: Finetune skeleton model\n",
    "    print(\"Phase 2: Finetuning skeleton model\")\n",
    "\n",
    "    # Load the best mask model before starting skeleton fine-tuning\n",
    "    model.load_state_dict(best_mask_model_weights)\n",
    "    model.prepare_for_skeleton_finetuning()\n",
    "\n",
    "    # Calculate class weights for skeleton\n",
    "    pos_weight = calculate_pos_weight(train_loader)\n",
    "    criterion_skeleton = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "    optimizer_skeleton = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    skeleton_phase_epochs = num_epochs - mask_phase_epochs\n",
    "    constant_lr_epochs = skeleton_phase_epochs // 2\n",
    "    cosine_annealing_epochs = skeleton_phase_epochs - constant_lr_epochs\n",
    "\n",
    "    scheduler_skeleton = torch.optim.lr_scheduler.LambdaLR(optimizer_skeleton, lambda epoch: 1)  # Constant LR\n",
    "\n",
    "    for epoch in tqdm(range(skeleton_phase_epochs), desc=\"Skeleton Finetuning\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, _, skeletons in train_loader:\n",
    "            images, skeletons = images.to(device), skeletons.to(device)\n",
    "\n",
    "            optimizer_skeleton.zero_grad()\n",
    "\n",
    "            outputs_skeleton = model(images)\n",
    "            loss = criterion_skeleton(outputs_skeleton, skeletons)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_skeleton.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Switch to cosine annealing after half the epochs\n",
    "        if epoch == constant_lr_epochs - 1:\n",
    "            scheduler_skeleton = CosineAnnealingLR(optimizer_skeleton, T_max=cosine_annealing_epochs)\n",
    "        scheduler_skeleton.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss, skeleton_iou = validate(model, val_loader, criterion_skeleton, device, task='skeleton')\n",
    "\n",
    "        print(f'Epoch {mask_phase_epochs + epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Skeleton IoU: {skeleton_iou:.4f}, '\n",
    "              f'LR: {scheduler_skeleton.get_last_lr()[0]:.6f}')\n",
    "\n",
    "        # Collect data for visualization\n",
    "        skeleton_train_losses.append(train_loss)\n",
    "        skeleton_val_losses.append(val_loss)\n",
    "        skeleton_ious.append(skeleton_iou)\n",
    "        skeleton_lrs.append(scheduler_skeleton.get_last_lr()[0])\n",
    "\n",
    "        if skeleton_iou > best_skeleton_iou:\n",
    "            best_skeleton_iou = skeleton_iou\n",
    "            best_skeleton_model_weights = model.state_dict().copy()\n",
    "            print(f'New best skeleton model saved with IoU: {best_skeleton_iou:.4f}')\n",
    "\n",
    "    # Collect all visualization data\n",
    "    visualization_data = {\n",
    "        'mask': {\n",
    "            'train_losses': mask_train_losses,\n",
    "            'val_losses': mask_val_losses,\n",
    "            'ious': mask_ious,\n",
    "            'lrs': mask_lrs\n",
    "        },\n",
    "        'skeleton': {\n",
    "            'train_losses': skeleton_train_losses,\n",
    "            'val_losses': skeleton_val_losses,\n",
    "            'ious': skeleton_ious,\n",
    "            'lrs': skeleton_lrs\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return model, best_mask_model_weights, best_skeleton_model_weights, visualization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hl6ZW5pJexK",
    "outputId": "6b35a058-c17b-4f7d-bcd6-eb066dc79db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Bne5dbfJexK",
    "outputId": "5affd140-2163-4792-bbdc-aa7f00050189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Phase 1: Training mask layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mask Training:   0%|          | 0/50 [01:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m DualTaskResNet(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 14\u001b[0m model, best_mask_weights, best_skeleton_weights, vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_finetuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Save the models\u001b[39;00m\n\u001b[0;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_mask_weights, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_mask_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 42\u001b[0m, in \u001b[0;36mtrain_model_with_finetuning\u001b[1;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     40\u001b[0m     optimizer_mask\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 42\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Switch to cosine annealing after half the epochs\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "image_dir = \"DATA/IMG\"\n",
    "mask_dir = \"DATA/MASK\"\n",
    "skeleton_dir = \"DATA/SKELETON\"\n",
    "batch_size = 32\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(image_dir, mask_dir, skeleton_dir, batch_size)\n",
    "\n",
    "model = DualTaskResNet(num_classes=1, pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "model, best_mask_weights, best_skeleton_weights, vis_data = train_model_with_finetuning(\n",
    "    model, train_loader, val_loader, num_epochs=100, device=device\n",
    ")\n",
    "\n",
    "# Save the best model\n",
    "# Save the models\n",
    "torch.save(best_mask_weights, 'best_mask_model.pth')\n",
    "torch.save(best_skeleton_weights, 'best_skeleton_model.pth')\n",
    "\n",
    "print(\"Training completed and best model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt8q2clLJexL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_training(visualization_data):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "\n",
    "    phases = ['mask', 'skeleton']\n",
    "    colors = ['blue', 'red']\n",
    "\n",
    "    for i, phase in enumerate(phases):\n",
    "        # Plot training and validation loss\n",
    "        axs[i, 0].plot(visualization_data[phase]['train_losses'], label='Train Loss', color=colors[0])\n",
    "        axs[i, 0].plot(visualization_data[phase]['val_losses'], label='Val Loss', color=colors[1])\n",
    "        axs[i, 0].set_title(f'{phase.capitalize()} Loss')\n",
    "        axs[i, 0].set_xlabel('Epoch')\n",
    "        axs[i, 0].set_ylabel('Loss')\n",
    "        axs[i, 0].legend()\n",
    "\n",
    "        # Plot IoU\n",
    "        axs[i, 1].plot(visualization_data[phase]['ious'], label='IoU', color=colors[0])\n",
    "        axs[i, 1].set_title(f'{phase.capitalize()} IoU')\n",
    "        axs[i, 1].set_xlabel('Epoch')\n",
    "        axs[i, 1].set_ylabel('IoU')\n",
    "        axs[i, 1].legend()\n",
    "\n",
    "        # Plot Learning Rate\n",
    "        axs[i, 2].plot(visualization_data[phase]['lrs'], label='Learning Rate', color=colors[0])\n",
    "        axs[i, 2].set_title(f'{phase.capitalize()} Learning Rate')\n",
    "        axs[i, 2].set_xlabel('Epoch')\n",
    "        axs[i, 2].set_ylabel('Learning Rate')\n",
    "        axs[i, 2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lESS3OPJexL"
   },
   "outputs": [],
   "source": [
    "# Visulize Training\n",
    "visualize_training(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjTaecfuJexL"
   },
   "outputs": [],
   "source": [
    "test_loader_for_test = test_loader\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mask_iou, test_skeleton_iou, test_avg_iou = validate(trained_model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Mask IoU: {test_mask_iou:.4f}, '\n",
    "        f'Test Skeleton IoU: {test_skeleton_iou:.4f}, Test Avg IoU: {test_avg_iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wc_vbqUXJexL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def generate_skeletons(model, test_dataloader, save_dir, device='cuda'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for inference\n",
    "        for i, batch in tqdm(enumerate(test_dataloader), desc=\"Generating Skeletons\"):\n",
    "            # Unpack only the images from the batch (ignore other values)\n",
    "            images = batch[0].to(device)\n",
    "\n",
    "            # Generate skeleton predictions\n",
    "            skeleton_preds = model(images, task='skeleton')\n",
    "\n",
    "            # Apply a threshold to get binary skeletons\n",
    "            skeleton_binary = (skeleton_preds > 0.5).float()\n",
    "\n",
    "            # Save each predicted skeleton\n",
    "            for j in range(skeleton_binary.size(0)):\n",
    "                save_path = os.path.join(save_dir, f\"skeleton_{i * test_dataloader.batch_size + j}.png\")\n",
    "                save_image(skeleton_binary[j], save_path)\n",
    "\n",
    "                print(f\"Skeleton saved at: {save_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi4wwGaSJexL"
   },
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model = DualTaskResNet(num_classes=1, pretrained=False)  # Adjust num_classes if necessary\n",
    "model.load_state_dict(torch.load(\"best_skeleton_model.pth\"))  # Load the trained weights\n",
    "model = model.to('cuda')  # Send the model to the GPU (or use 'cpu')\n",
    "\n",
    "# Load your test dataloader (assumes test_dataloader is defined)\n",
    "# Example: test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Directory where skeletons will be saved\n",
    "save_dir = \"./skeleton_predictions\"\n",
    "\n",
    "# Call the skeleton generation function\n",
    "generate_skeletons(model, test_loader, save_dir, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWu8eZ-KJexL"
   },
   "outputs": [],
   "source": [
    "for _, _, skeletons in train_loader:\n",
    "    print(skeletons.min(), skeletons.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYD6NJAGVOvP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
