{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTaskResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(DualTaskResNet, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        self.resnet18 = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.layer0 = nn.Sequential(*list(self.resnet18.children())[:3])\n",
    "        self.layer1 = nn.Sequential(*list(self.resnet18.children())[3:5])\n",
    "        self.layer2 = self.resnet18[5]\n",
    "        self.layer3 = self.resnet18[6]\n",
    "        self.layer4 = self.resnet18[7]\n",
    "\n",
    "        # Dilated convolutions for layer2 (reduced to 3)\n",
    "        self.dilation_conv1_l2 = self._make_dilated_conv(128, 256, 2)\n",
    "        self.dilation_conv2_l2 = self._make_dilated_conv(128, 256, 4)\n",
    "        self.dilation_conv3_l2 = self._make_dilated_conv(128, 256, 8)\n",
    "\n",
    "        # Dilated convolutions for layer3 (reduced to 3)\n",
    "        self.dilation_conv1_l3 = self._make_dilated_conv(256, 512, 2)\n",
    "        self.dilation_conv2_l3 = self._make_dilated_conv(256, 512, 4)\n",
    "        self.dilation_conv3_l3 = self._make_dilated_conv(256, 512, 8)\n",
    "\n",
    "        # New: Dilated convolutions for layer4\n",
    "        self.dilation_conv1_l4 = self._make_dilated_conv(512, 1024, 2)\n",
    "        self.dilation_conv2_l4 = self._make_dilated_conv(512, 1024, 4)\n",
    "        self.dilation_conv3_l4 = self._make_dilated_conv(512, 1024, 8)\n",
    "\n",
    "        # Upsampling path (adjusted for new dimensions)\n",
    "        self.upsample1 = self._make_transpose_conv(3072, 512, 2)  # 7x7 -> 14x14\n",
    "        self.upsample2 = self._make_transpose_conv(2048, 512, 2)  # 14x14 -> 28x28\n",
    "        self.upsample3 = self._make_transpose_conv(1280, 256, 2)  # 28x28 -> 56x56\n",
    "        self.upsample4 = self._make_transpose_conv(256, 128, 2)  # 56x56 -> 112x112\n",
    "        self.upsample5 = self._make_transpose_conv(128, 64, 2)  # 112x112 -> 224x224\n",
    "        self.convf = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "        # Task-specific output layers\n",
    "        self.mask_output = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.skeleton_output = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "\n",
    "    def _make_dilated_conv(self, in_channels, out_channels, dilation):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _make_transpose_conv(self, in_channels, out_channels, scale_factor):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=scale_factor, padding=0, output_padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, task='mask'):\n",
    "        # Expected input size: 224x224x3\n",
    "        layer0 = self.layer0(img)  # 112x112x64\n",
    "        layer1 = self.layer1(layer0)  # 56x56x64\n",
    "        layer2 = self.layer2(layer1)  # 28x28x128\n",
    "        layer3 = self.layer3(layer2)  # 14x14x256\n",
    "        layer4 = self.layer4(layer3)  # 7x7x512\n",
    "\n",
    "        # Apply dilation to layer2 (28x28x128)\n",
    "        y1 = self.dilation_conv1_l2(layer2)\n",
    "        y2 = self.dilation_conv2_l2(layer2)\n",
    "        y3 = self.dilation_conv3_l2(layer2)\n",
    "        y = torch.cat([y1, y2, y3], dim=1)  # 28x28x768\n",
    "\n",
    "        # Apply dilation to layer3 (14x14x256)\n",
    "        z1 = self.dilation_conv1_l3(layer3)\n",
    "        z2 = self.dilation_conv2_l3(layer3)\n",
    "        z3 = self.dilation_conv3_l3(layer3)\n",
    "        z = torch.cat([z1, z2, z3], dim=1)  # 14x14x1536\n",
    "\n",
    "        # Apply dilation to layer4 (7x7x512)\n",
    "        w1 = self.dilation_conv1_l4(layer4)\n",
    "        w2 = self.dilation_conv2_l4(layer4)\n",
    "        w3 = self.dilation_conv3_l4(layer4)\n",
    "        w = torch.cat([w1, w2, w3], dim=1)  # 7x7x3072\n",
    "\n",
    "        # Upsampling path\n",
    "        x = self.upsample1(w)  # 14x14x512\n",
    "        x = torch.cat([x, z], dim=1)  # 14x14x2048\n",
    "        x = self.upsample2(x)  # 28x28x512\n",
    "        x = torch.cat([x, y], dim=1)  # 28x28x1280\n",
    "        x = self.upsample3(x)  # 56x56x256\n",
    "        x = self.upsample4(x)  # 112x112x128\n",
    "        x = self.upsample5(x)  # 224x224x64\n",
    "        x = self.convf(x)  # 224x224xnum_classes\n",
    "\n",
    "        if task == 'mask':\n",
    "            output = self.mask_output(x)\n",
    "        elif task == 'skeleton':\n",
    "            output = self.skeleton_output(x)\n",
    "        else:\n",
    "            raise ValueError(\"Task must be either 'mask' or 'skeleton'\")\n",
    "\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_dualtask_resnet():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model instance\n",
    "    model = DualTaskResNet(num_classes=1, pretrained=False).to(device)\n",
    "\n",
    "    # Print model summary\n",
    "    summary(model, (3, 224, 224))\n",
    "\n",
    "    # Generate random input\n",
    "    batch_size = 4\n",
    "    input_tensor = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "    # Test mask generation\n",
    "    print(\"\\nTesting mask generation:\")\n",
    "    mask_output = model(input_tensor, task='mask')\n",
    "    print(f\"Mask output shape: {mask_output.shape}\")\n",
    "    print(f\"Mask output min: {mask_output.min().item():.4f}, max: {mask_output.max().item():.4f}\")\n",
    "\n",
    "    # Test skeletonization\n",
    "    print(\"\\nTesting skeletonization:\")\n",
    "    skeleton_output = model(input_tensor, task='skeleton')\n",
    "    print(f\"Skeleton output shape: {skeleton_output.shape}\")\n",
    "    print(f\"Skeleton output min: {skeleton_output.min().item():.4f}, max: {skeleton_output.max().item():.4f}\")\n",
    "\n",
    "    # Test if outputs are different\n",
    "    print(\"\\nChecking if mask and skeleton outputs are different:\")\n",
    "    is_different = not torch.allclose(mask_output, skeleton_output)\n",
    "    print(f\"Outputs are different: {is_different}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-2         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "       BatchNorm2d-4         [-1, 64, 112, 112]             128\n",
      "              ReLU-5         [-1, 64, 112, 112]               0\n",
      "              ReLU-6         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-7           [-1, 64, 56, 56]               0\n",
      "         MaxPool2d-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-21           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-24           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
      "             ReLU-27           [-1, 64, 56, 56]               0\n",
      "             ReLU-28           [-1, 64, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "             ReLU-34           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-35           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]          73,728\n",
      "           Conv2d-38          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "             ReLU-41          [-1, 128, 28, 28]               0\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-44          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "           Conv2d-47          [-1, 128, 28, 28]           8,192\n",
      "           Conv2d-48          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "             ReLU-52          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-53          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "             ReLU-60          [-1, 128, 28, 28]               0\n",
      "           Conv2d-61          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "             ReLU-66          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-67          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 14, 14]         294,912\n",
      "           Conv2d-70          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-71          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-72          [-1, 256, 14, 14]             512\n",
      "             ReLU-73          [-1, 256, 14, 14]               0\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-76          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-77          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "           Conv2d-79          [-1, 256, 14, 14]          32,768\n",
      "           Conv2d-80          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "             ReLU-83          [-1, 256, 14, 14]               0\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-86          [-1, 256, 14, 14]               0\n",
      "           Conv2d-87          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-88          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "             ReLU-92          [-1, 256, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-96          [-1, 256, 14, 14]             512\n",
      "             ReLU-97          [-1, 256, 14, 14]               0\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-100          [-1, 256, 14, 14]               0\n",
      "          Conv2d-101            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-102            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-103            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-105            [-1, 512, 7, 7]               0\n",
      "            ReLU-106            [-1, 512, 7, 7]               0\n",
      "          Conv2d-107            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-108            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-111            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-112            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-115            [-1, 512, 7, 7]               0\n",
      "            ReLU-116            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-117            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-118            [-1, 512, 7, 7]               0\n",
      "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-120            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-121            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-122            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-123            [-1, 512, 7, 7]               0\n",
      "            ReLU-124            [-1, 512, 7, 7]               0\n",
      "          Conv2d-125            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-126            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-127            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-128            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-129            [-1, 512, 7, 7]               0\n",
      "            ReLU-130            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-131            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-132            [-1, 512, 7, 7]               0\n",
      "          Conv2d-133          [-1, 256, 28, 28]         295,168\n",
      "     BatchNorm2d-134          [-1, 256, 28, 28]             512\n",
      "            ReLU-135          [-1, 256, 28, 28]               0\n",
      "          Conv2d-136          [-1, 256, 28, 28]         295,168\n",
      "     BatchNorm2d-137          [-1, 256, 28, 28]             512\n",
      "            ReLU-138          [-1, 256, 28, 28]               0\n",
      "          Conv2d-139          [-1, 256, 28, 28]         295,168\n",
      "     BatchNorm2d-140          [-1, 256, 28, 28]             512\n",
      "            ReLU-141          [-1, 256, 28, 28]               0\n",
      "          Conv2d-142          [-1, 512, 14, 14]       1,180,160\n",
      "     BatchNorm2d-143          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-144          [-1, 512, 14, 14]               0\n",
      "          Conv2d-145          [-1, 512, 14, 14]       1,180,160\n",
      "     BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-147          [-1, 512, 14, 14]               0\n",
      "          Conv2d-148          [-1, 512, 14, 14]       1,180,160\n",
      "     BatchNorm2d-149          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-150          [-1, 512, 14, 14]               0\n",
      "          Conv2d-151           [-1, 1024, 7, 7]       4,719,616\n",
      "     BatchNorm2d-152           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-153           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1024, 7, 7]       4,719,616\n",
      "     BatchNorm2d-155           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-156           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-157           [-1, 1024, 7, 7]       4,719,616\n",
      "     BatchNorm2d-158           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-159           [-1, 1024, 7, 7]               0\n",
      " ConvTranspose2d-160          [-1, 512, 14, 14]       6,291,968\n",
      "     BatchNorm2d-161          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-162          [-1, 512, 14, 14]               0\n",
      " ConvTranspose2d-163          [-1, 512, 28, 28]       4,194,816\n",
      "     BatchNorm2d-164          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-165          [-1, 512, 28, 28]               0\n",
      " ConvTranspose2d-166          [-1, 256, 56, 56]       1,310,976\n",
      "     BatchNorm2d-167          [-1, 256, 56, 56]             512\n",
      "            ReLU-168          [-1, 256, 56, 56]               0\n",
      " ConvTranspose2d-169        [-1, 128, 112, 112]         131,200\n",
      "     BatchNorm2d-170        [-1, 128, 112, 112]             256\n",
      "            ReLU-171        [-1, 128, 112, 112]               0\n",
      " ConvTranspose2d-172         [-1, 64, 224, 224]          32,832\n",
      "     BatchNorm2d-173         [-1, 64, 224, 224]             128\n",
      "            ReLU-174         [-1, 64, 224, 224]               0\n",
      "          Conv2d-175          [-1, 1, 224, 224]              65\n",
      "          Conv2d-176          [-1, 1, 224, 224]               2\n",
      "================================================================\n",
      "Total params: 52,913,411\n",
      "Trainable params: 52,913,411\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 290.55\n",
      "Params size (MB): 201.85\n",
      "Estimated Total Size (MB): 492.98\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Testing mask generation:\n",
      "Mask output shape: torch.Size([4, 1, 224, 224])\n",
      "Mask output min: 0.1652, max: 0.5258\n",
      "\n",
      "Testing skeletonization:\n",
      "Skeleton output shape: torch.Size([4, 1, 224, 224])\n",
      "Skeleton output min: 0.2532, max: 0.7978\n",
      "\n",
      "Checking if mask and skeleton outputs are different:\n",
      "Outputs are different: True\n"
     ]
    }
   ],
   "source": [
    "test_dualtask_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
